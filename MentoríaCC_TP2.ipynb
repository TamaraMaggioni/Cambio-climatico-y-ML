{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cd4483",
   "metadata": {},
   "source": [
    "# MENTORÍA 06\n",
    "# **Cambio climático y ML: cómo mitigar las emisiones de CO2 mediante la reducción del consumo energético en construcciones edilicias**\n",
    "\n",
    "**TP N°2: Análisis exploratorio y curación**\n",
    "\n",
    "Fecha de entrega: **29 de julio**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491c70d",
   "metadata": {},
   "source": [
    "## Consignas generales\n",
    "\n",
    "En este segundo TP deberemos asegurarnos de que nuestro dataset esté listo para introducirlo a un modelo. \n",
    "Al igual que en el TP anterior, les propongo la elaboración de un informe donde se aclare o se detalle las decisiones tomadas.\n",
    "Podemos tener más de un set de datos (o escenarios), de forma tal de poder analizar combinaciones diferentes de features que creamos puedan tener incidencia en nuestras predicciones. Les propongo contar con al menos dos escenarios (pueden evaluar más de dos, por supuesto):\n",
    "\n",
    "    - Escenario base: datos sin escalar y con todas las features.\n",
    "    - Escenario 2: datos escalados y con las mejoras que hayan realizado.\n",
    "    \n",
    "De esta manera, podremos comparar qué tan buenas fueron nuestras elecciones, al contrastar las métricas que obtendremos con el peor de los modelos que podríamos haber usado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5753d",
   "metadata": {},
   "source": [
    "## Pasos a seguir para evitar el train-test data leaking\n",
    "\n",
    "1- Renombrar columnas o transformar unidades de medición.\n",
    "\n",
    "2- Chequear datos repetidos.\n",
    "\n",
    "3- Chequear \"missingness\" o valores faltantes:\n",
    " \n",
    "Verificar que las columnas numéricas correspondan a un tipo de dato numérico. Muchas veces, los valores faltantes se llenan con algún string como \"NA\", '.', o \"-\". En ese caso, un dato numérico (float o int) tendrá el tipo object.\n",
    "    \n",
    "        1- df.info() --> chequeo tipo de datos\n",
    "        2- df.coldeinteres.unique() --> si veo una columna misteriosa, analizo los valores que contiene\n",
    "        3- df.describe --> Si con un describe (o un value_counts) identifican valores == 0 en features que no deberían tener 0, entonces, ahí hay otro dato falante incorporado como 0. \n",
    " \n",
    "Si encuentran algo así, deben reemplazar esos valores con \"missing value\" o NaN:\n",
    "\n",
    "        1- df = pd.read_csv('data.csv', na_values='-')\n",
    "        2- df.replace('-', np.nan)\n",
    "        2- df.coldeinteres[df.coldeinteres == 0] = np.nan\n",
    "\n",
    "Si deciden eliminar alguna columna por presentar muchos valores faltantes, pueden hacerlo en esta etapa. \n",
    "\n",
    "Pero, si deciden imputar, deberán hacerlo más adelante.\n",
    "    \n",
    "----------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Los puntos anteriores los pueden realizar con la totalidad del dataset. Sin embargo, previo a trabajar con las variables numéricas o cuantitativas que requieran escalado, chequeo de outliers, o evaluación de valores faltantes para imputar, debemos **DIVIDIR EL DATA SET EN TRAIN Y TEST**. De lo contrario, estaríamos comentiendo lo que se conoce como **contaminación train-test (un tipo de data leaking).** \n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "4- Separar la variable target y dividir el set de datos en train y test.\n",
    "\n",
    "        X = df.drop(['target'],axis=1)\n",
    "        y = df['target']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)        \n",
    "        \n",
    "5- Luego de identificar las variables categóricas o cualitativas, evaluar su distribución, reasignarlas (o no), encodearlas por separado. Pueden usar getdummies, OneHotEncoding o OrdinalEncoding, dependiendo el objetivo.\n",
    "Durante el proceso, recordar dropear una de las categorías para evitar la multicolinealidad.\n",
    "    \n",
    "        X_train = pd.get_dummies(X_train, columns=[\"catcol1\", \"catcol2\"], drop_first=True)\n",
    "        X_test = pd.get_dummies(X_test, columns=[\"catcol1\", \"catcol2\"], drop_first=True)\n",
    "        \n",
    "6- Escalar variables numéricas con el método .fit_transform en el set de entrenamiento, pero sólo con el método .transform en el set de testeo. Esto se hace para utilizar la media y el desvío calculados (si usamos StandardScaler, por ejemplo) únicamente con los datos de entrenamiento. Recuerden que los datos de test son datos que simulan provenir de la realidad, posterior al entrenamiento del modelo, y su función radica en testear que tan bien generaliza mi modelo con datos nuevos (ya que para eso estamos haciendo y entrenando nuestros modelos: para que predigan lo mejor posible sobre nuevos valores de features que nunca vio). Ejemplos:\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit_transform(X_train)\n",
    "        scaler.transform(X_test)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        num_cols = [\"numcol1\",\"numcol2\",\"numcol3\"]\n",
    "        X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "        X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "7- El proceso de imputar es similar al de escalado:\n",
    "\n",
    "        # Imputación\n",
    "        my_imputer = SimpleImputer()\n",
    "        imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "        imputed_X_test = pd.DataFrame(my_imputer.transform(X_test))\n",
    "\n",
    "        # Como la imputación elimina los nombres de las columnas, hay que agregarlos de nuevo\n",
    "        imputed_X_train.columns = X_train.columns\n",
    "        imputed_X_test.columns = X_test.columns\n",
    "        \n",
    "-------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "En cuanto al análisis de los outliers, va a depender si los necesitamos para corregir valores atípicos (temperaturas de 1000°C, por ejemplo) o si vamos a depender del análisis de la distribución de los datos y, por ende, de la mediana. En el primer caso, podemos usar todo el dataset. Pero en el segundo, deberíamos trabajar el set de train y le de test por separado. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc7b94",
   "metadata": {},
   "source": [
    "Fuentes:\n",
    "\n",
    "https://jahazielponce.com/kaggle-30-dias-ml-dias-12-14/\n",
    "\n",
    "https://towardsdatascience.com/the-dreaded-antagonist-data-leakage-in-machine-learning-5f08679852cc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
